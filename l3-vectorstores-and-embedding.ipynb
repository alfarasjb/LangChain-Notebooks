{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import openai \n",
    "from dotenv import load_dotenv \n",
    "\n",
    "load_dotenv() \n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load PDF\n",
    "loaders = [\n",
    "    # Duplicate documents on purpose - messy data\n",
    "    PyPDFLoader(\"data/MachineLearning-Lecture01.pdf\"),\n",
    "    PyPDFLoader(\"data/MachineLearning-Lecture01.pdf\"),\n",
    "    PyPDFLoader(\"data/MachineLearning-Lecture02.pdf\"),\n",
    "    PyPDFLoader(\"data/MachineLearning-Lecture03.pdf\"),\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Embeddings \n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"i like dogs\"\n",
    "sentence2 = \"i like canines\"\n",
    "sentence3 = \"the weather is ugly outside\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding1 = embedding.embed_query(sentence1)\n",
    "embedding2 = embedding.embed_query(sentence2)\n",
    "embedding3 = embedding.embed_query(sentence3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9630351468341318"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dot product to check similarity\n",
    "import numpy as np\n",
    "\n",
    "np.dot(embedding1, embedding2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7701147942700534"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(embedding1, embedding3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7591130371091992"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(embedding2, embedding3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = PineconeVectorStore.from_documents(\n",
    "    docs, embedding=embedding, index_name=\"deeplearningai-langchain\"\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 8.0, 'source': 'data/MachineLearning-Lecture01.pdf'}, page_content='those homeworks will be done in either MATLA B or in Octave, which is sort of — I \\nknow some people call it a free ve rsion of MATLAB, which it sort  of is, sort of isn\\'t.  \\nSo I guess for those of you that haven\\'t s een MATLAB before, and I know most of you \\nhave, MATLAB is I guess part of the programming language that makes it very easy to write codes using matrices, to write code for numerical routines, to move data around, to \\nplot data. And it\\'s sort of an extremely easy to  learn tool to use for implementing a lot of \\nlearning algorithms.  \\nAnd in case some of you want to work on your  own home computer or something if you \\ndon\\'t have a MATLAB license, for the purposes of  this class, there\\'s also — [inaudible] \\nwrite that down [inaudible] MATLAB — there\\' s also a software package called Octave \\nthat you can download for free off the Internet. And it has somewhat fewer features than MATLAB, but it\\'s free, and for the purposes of  this class, it will work for just about \\neverything.  \\nSo actually I, well, so yeah, just a side comment for those of you that haven\\'t seen \\nMATLAB before I guess, once a colleague of mine at a different university, not at \\nStanford, actually teaches another machine l earning course. He\\'s taught it for many years. \\nSo one day, he was in his office, and an old student of his from, lik e, ten years ago came \\ninto his office and he said, \"Oh, professo r, professor, thank you so much for your \\nmachine learning class. I learned so much from it. There\\'s this stuff that I learned in your \\nclass, and I now use every day. And it\\'s help ed me make lots of money, and here\\'s a \\npicture of my big house.\"  \\nSo my friend was very excited. He said, \"W ow. That\\'s great. I\\'m glad to hear this \\nmachine learning stuff was actually useful. So what was it that you learned? Was it \\nlogistic regression? Was it the PCA? Was it the data ne tworks? What was it that you \\nlearned that was so helpful?\" And the student said, \"Oh, it was the MATLAB.\"  \\nSo for those of you that don\\'t know MATLAB yet, I hope you do learn it. It\\'s not hard, \\nand we\\'ll actually have a short MATLAB tutori al in one of the discussion sections for \\nthose of you that don\\'t know it.  \\nOkay. The very last piece of logistical th ing is the discussion s ections. So discussion \\nsections will be taught by the TAs, and atte ndance at discussion sections is optional, \\nalthough they\\'ll also be recorded and televi sed. And we\\'ll use the discussion sections \\nmainly for two things. For the next two or th ree weeks, we\\'ll use the discussion sections \\nto go over the prerequisites to this class or if some of you haven\\'t seen probability or \\nstatistics for a while or maybe algebra, we\\'ll go over those in the discussion sections as a \\nrefresher for those of you that want one.  \\nLater in this quarter, we\\'ll also use the disc ussion sections to go over extensions for the \\nmaterial that I\\'m teaching in the main lectur es. So machine learning is a huge field, and \\nthere are a few extensions that we really want  to teach but didn\\'t have time in the main \\nlectures for.  '),\n",
       " Document(metadata={'page': 8.0, 'source': 'data/MachineLearning-Lecture01.pdf'}, page_content='those homeworks will be done in either MATLA B or in Octave, which is sort of — I \\nknow some people call it a free ve rsion of MATLAB, which it sort  of is, sort of isn\\'t.  \\nSo I guess for those of you that haven\\'t s een MATLAB before, and I know most of you \\nhave, MATLAB is I guess part of the programming language that makes it very easy to write codes using matrices, to write code for numerical routines, to move data around, to \\nplot data. And it\\'s sort of an extremely easy to  learn tool to use for implementing a lot of \\nlearning algorithms.  \\nAnd in case some of you want to work on your  own home computer or something if you \\ndon\\'t have a MATLAB license, for the purposes of  this class, there\\'s also — [inaudible] \\nwrite that down [inaudible] MATLAB — there\\' s also a software package called Octave \\nthat you can download for free off the Internet. And it has somewhat fewer features than MATLAB, but it\\'s free, and for the purposes of  this class, it will work for just about \\neverything.  \\nSo actually I, well, so yeah, just a side comment for those of you that haven\\'t seen \\nMATLAB before I guess, once a colleague of mine at a different university, not at \\nStanford, actually teaches another machine l earning course. He\\'s taught it for many years. \\nSo one day, he was in his office, and an old student of his from, lik e, ten years ago came \\ninto his office and he said, \"Oh, professo r, professor, thank you so much for your \\nmachine learning class. I learned so much from it. There\\'s this stuff that I learned in your \\nclass, and I now use every day. And it\\'s help ed me make lots of money, and here\\'s a \\npicture of my big house.\"  \\nSo my friend was very excited. He said, \"W ow. That\\'s great. I\\'m glad to hear this \\nmachine learning stuff was actually useful. So what was it that you learned? Was it \\nlogistic regression? Was it the PCA? Was it the data ne tworks? What was it that you \\nlearned that was so helpful?\" And the student said, \"Oh, it was the MATLAB.\"  \\nSo for those of you that don\\'t know MATLAB yet, I hope you do learn it. It\\'s not hard, \\nand we\\'ll actually have a short MATLAB tutori al in one of the discussion sections for \\nthose of you that don\\'t know it.  \\nOkay. The very last piece of logistical th ing is the discussion s ections. So discussion \\nsections will be taught by the TAs, and atte ndance at discussion sections is optional, \\nalthough they\\'ll also be recorded and televi sed. And we\\'ll use the discussion sections \\nmainly for two things. For the next two or th ree weeks, we\\'ll use the discussion sections \\nto go over the prerequisites to this class or if some of you haven\\'t seen probability or \\nstatistics for a while or maybe algebra, we\\'ll go over those in the discussion sections as a \\nrefresher for those of you that want one.  \\nLater in this quarter, we\\'ll also use the disc ussion sections to go over extensions for the \\nmaterial that I\\'m teaching in the main lectur es. So machine learning is a huge field, and \\nthere are a few extensions that we really want  to teach but didn\\'t have time in the main \\nlectures for.  '),\n",
       " Document(metadata={'page': 8.0, 'source': 'data/MachineLearning-Lecture01.pdf'}, page_content='those homeworks will be done in either MATLA B or in Octave, which is sort of — I \\nknow some people call it a free ve rsion of MATLAB, which it sort  of is, sort of isn\\'t.  \\nSo I guess for those of you that haven\\'t s een MATLAB before, and I know most of you \\nhave, MATLAB is I guess part of the programming language that makes it very easy to write codes using matrices, to write code for numerical routines, to move data around, to \\nplot data. And it\\'s sort of an extremely easy to  learn tool to use for implementing a lot of \\nlearning algorithms.  \\nAnd in case some of you want to work on your  own home computer or something if you \\ndon\\'t have a MATLAB license, for the purposes of  this class, there\\'s also — [inaudible] \\nwrite that down [inaudible] MATLAB — there\\' s also a software package called Octave \\nthat you can download for free off the Internet. And it has somewhat fewer features than MATLAB, but it\\'s free, and for the purposes of  this class, it will work for just about \\neverything.  \\nSo actually I, well, so yeah, just a side comment for those of you that haven\\'t seen \\nMATLAB before I guess, once a colleague of mine at a different university, not at \\nStanford, actually teaches another machine l earning course. He\\'s taught it for many years. \\nSo one day, he was in his office, and an old student of his from, lik e, ten years ago came \\ninto his office and he said, \"Oh, professo r, professor, thank you so much for your \\nmachine learning class. I learned so much from it. There\\'s this stuff that I learned in your \\nclass, and I now use every day. And it\\'s help ed me make lots of money, and here\\'s a \\npicture of my big house.\"  \\nSo my friend was very excited. He said, \"W ow. That\\'s great. I\\'m glad to hear this \\nmachine learning stuff was actually useful. So what was it that you learned? Was it \\nlogistic regression? Was it the PCA? Was it the data ne tworks? What was it that you \\nlearned that was so helpful?\" And the student said, \"Oh, it was the MATLAB.\"  \\nSo for those of you that don\\'t know MATLAB yet, I hope you do learn it. It\\'s not hard, \\nand we\\'ll actually have a short MATLAB tutori al in one of the discussion sections for \\nthose of you that don\\'t know it.  \\nOkay. The very last piece of logistical th ing is the discussion s ections. So discussion \\nsections will be taught by the TAs, and atte ndance at discussion sections is optional, \\nalthough they\\'ll also be recorded and televi sed. And we\\'ll use the discussion sections \\nmainly for two things. For the next two or th ree weeks, we\\'ll use the discussion sections \\nto go over the prerequisites to this class or if some of you haven\\'t seen probability or \\nstatistics for a while or maybe algebra, we\\'ll go over those in the discussion sections as a \\nrefresher for those of you that want one.  \\nLater in this quarter, we\\'ll also use the disc ussion sections to go over extensions for the \\nmaterial that I\\'m teaching in the main lectur es. So machine learning is a huge field, and \\nthere are a few extensions that we really want  to teach but didn\\'t have time in the main \\nlectures for.  '),\n",
       " Document(metadata={'page': 8.0, 'source': 'data/MachineLearning-Lecture01.pdf'}, page_content='those homeworks will be done in either MATLA B or in Octave, which is sort of — I \\nknow some people call it a free ve rsion of MATLAB, which it sort  of is, sort of isn\\'t.  \\nSo I guess for those of you that haven\\'t s een MATLAB before, and I know most of you \\nhave, MATLAB is I guess part of the programming language that makes it very easy to write codes using matrices, to write code for numerical routines, to move data around, to \\nplot data. And it\\'s sort of an extremely easy to  learn tool to use for implementing a lot of \\nlearning algorithms.  \\nAnd in case some of you want to work on your  own home computer or something if you \\ndon\\'t have a MATLAB license, for the purposes of  this class, there\\'s also — [inaudible] \\nwrite that down [inaudible] MATLAB — there\\' s also a software package called Octave \\nthat you can download for free off the Internet. And it has somewhat fewer features than MATLAB, but it\\'s free, and for the purposes of  this class, it will work for just about \\neverything.  \\nSo actually I, well, so yeah, just a side comment for those of you that haven\\'t seen \\nMATLAB before I guess, once a colleague of mine at a different university, not at \\nStanford, actually teaches another machine l earning course. He\\'s taught it for many years. \\nSo one day, he was in his office, and an old student of his from, lik e, ten years ago came \\ninto his office and he said, \"Oh, professo r, professor, thank you so much for your \\nmachine learning class. I learned so much from it. There\\'s this stuff that I learned in your \\nclass, and I now use every day. And it\\'s help ed me make lots of money, and here\\'s a \\npicture of my big house.\"  \\nSo my friend was very excited. He said, \"W ow. That\\'s great. I\\'m glad to hear this \\nmachine learning stuff was actually useful. So what was it that you learned? Was it \\nlogistic regression? Was it the PCA? Was it the data ne tworks? What was it that you \\nlearned that was so helpful?\" And the student said, \"Oh, it was the MATLAB.\"  \\nSo for those of you that don\\'t know MATLAB yet, I hope you do learn it. It\\'s not hard, \\nand we\\'ll actually have a short MATLAB tutori al in one of the discussion sections for \\nthose of you that don\\'t know it.  \\nOkay. The very last piece of logistical th ing is the discussion s ections. So discussion \\nsections will be taught by the TAs, and atte ndance at discussion sections is optional, \\nalthough they\\'ll also be recorded and televi sed. And we\\'ll use the discussion sections \\nmainly for two things. For the next two or th ree weeks, we\\'ll use the discussion sections \\nto go over the prerequisites to this class or if some of you haven\\'t seen probability or \\nstatistics for a while or maybe algebra, we\\'ll go over those in the discussion sections as a \\nrefresher for those of you that want one.  \\nLater in this quarter, we\\'ll also use the disc ussion sections to go over extensions for the \\nmaterial that I\\'m teaching in the main lectur es. So machine learning is a huge field, and \\nthere are a few extensions that we really want  to teach but didn\\'t have time in the main \\nlectures for.  ')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What did they say about matlab?\"\n",
    "vectorstore.similarity_search(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 0.0, 'source': 'data/MachineLearning-Lecture03.pdf'}, page_content='MachineLearning-Lecture03  \\nInstructor (Andrew Ng) :Okay. Good morning and welcome b ack to the third lecture of \\nthis class. So here’s what I want to do t oday, and some of the topics I do today may seem \\na little bit like I’m jumping, sort  of, from topic to topic, but here’s, sort of, the outline for \\ntoday and the illogical flow of ideas. In the last lecture, we  talked about linear regression \\nand today I want to talk about sort of an  adaptation of that called locally weighted \\nregression. It’s very a popular  algorithm that’s actually one of my former mentors \\nprobably favorite machine learning algorithm.  \\nWe’ll then talk about a probabl e second interpretation of linear regression and use that to \\nmove onto our first classification algorithm, which is logistic regr ession; take a brief \\ndigression to tell you about something cal led the perceptron algorithm, which is \\nsomething we’ll come back to, again, later this  quarter; and time allowing I hope to get to \\nNewton’s method, which is an algorithm fo r fitting logistic regression models.  \\nSo this is recap where we’re talking about in the previous lecture, remember the notation \\nI defined was that I used this X superscrip t I, Y superscript I to denote the I training \\nexample. And when we’re talking about linear regression or linear l east squares, we use \\nthis to denote the predicted value of “by my hypothesis H” on the input XI. And my \\nhypothesis was franchised by the vector of gram s as theta and so we said that this was \\nequal to some from theta J, si J, and more  theta transpose X. And we had the convention \\nthat X subscript Z is equal to one so this accounts for the intercept term in our linear regression model. And lowercas e n here was the notation I was using for the number of \\nfeatures in my training set. Okay? So in  the example when trying to predict housing \\nprices, we had two features, the size of the house and the number of bedrooms. We had \\ntwo features and there was – li ttle n was equal to two. So just to finish recapping the \\nprevious lecture, we define d this quadratic cos function J of theta equals one-half, \\nsomething I equals one to m, theta of XI mi nus YI squared where this is the sum over our \\nm training examples and my training set. So lowercase m was the notation I’ve been \\nusing to denote the number of training exampl es I have and the size of my training set. \\nAnd at the end of the last lecture, we deri ve the value of theta that minimizes this \\nenclosed form, which was X transpose X inverse X transpose Y. Okay?  \\nSo as we move on in today’s lecture, I’ll cont inue to use this notation and, again, I realize \\nthis is a fair amount of notation to all reme mber, so if partway th rough this lecture you \\nforgot – if you’re having trouble remembering what lowercase m is or what lowercase n \\nis or something please raise your hand and ask. When we talked about linear regression \\nlast time we used two features. One of the f eatures was the size of the houses in square \\nfeet, so the living area of the house, and the other feature was the number of bedrooms in \\nthe house. In general, we apply a machine- learning algorithm to some problem that you \\ncare about. The choice of the features will very much be up to you, right? And the way \\nyou choose your features to give the learning algorithm will often have a large impact on \\nhow it actually does. So just for example, the choice we made last time was X1 equal this \\nsize, and let’s leave this idea of the feature of the number of bedrooms for now, let’s say \\nwe don’t have data that tells us how many bedrooms are in these houses. One thing you '),\n",
       " Document(metadata={'page': 14.0, 'source': 'data/MachineLearning-Lecture03.pdf'}, page_content='Student: It’s the lowest it –  \\nInstructor (Andrew Ng) :No, exactly. Right. So zero to the same, this is not the same, \\nright? And the reason is, in logi stic regression this is diffe rent from before, right? The \\ndefinition of this H subscript theta of XI is not the same as the definition I was using in \\nthe previous lecture. And in pa rticular this is no longer thet a transpose XI. This is not a \\nlinear function anymore. This is  a logistic function of theta transpose XI. Okay? So even \\nthough this looks cosmetically similar, even though this is similar on the surface, to the \\nBastrian descent rule I derive d last time for least squares regression this is actually a \\ntotally different learning algorithm. Okay? And it turns out that there’s actually no \\ncoincidence that you ended up with the same l earning rule. We’ll actually talk a bit more \\nabout this later when we talk about generalized linear models. But this is one of the most \\nelegant generalized learning models that we’l l see later. That even though we’re using a \\ndifferent model, you actually ended up with wh at looks like the sa me learning algorithm \\nand it’s actually no coincidence. Cool.  \\nOne last comment as part of a sort of l earning process, over here I said I take the \\nderivatives and I ended up with this line . I didn’t want to make you sit through a long \\nalgebraic derivation, but later t oday or later this week, pleas e, do go home and look at our \\nlecture notes, where I wrote out the entirety of  this derivation in full, and make sure you \\ncan follow every single step of how we take partial derivatives of this log likelihood to \\nget this formula over here. Okay? By the way, for those who are inte rested in seriously \\nmasking machine learning material, when you go home and look at the lecture notes it \\nwill actually be very easy for most of you to look through the lecture notes and read \\nthrough every line and go yep, that makes sense,  that makes sense, that makes sense, and, \\nsort of, say cool. I see how you get this line. You want to make sure you really \\nunderstand the material. My concrete suggesti on to you would be to you to go home, read \\nthrough the lecture notes, check every line, and then to cover up the derivation and see if \\nyou can derive this example, right? So in general, that’s usually good advice for studying \\ntechnical material like machine learning. Wh ich is if you work through a proof and you \\nthink you understood every line, the way to make sure you rea lly understood it is to cover \\nit up and see if you can rederive the entire th ing itself. This is actually a great way \\nbecause I did this a lot when I was trying to  study various pieces of machine learning \\ntheory and various proofs. And this is actua lly a great way to study because cover up the \\nderivations and see if you can do it yourself without looking at the origin al derivation. All \\nright.  \\nI probably won’t get to Newton’s Method toda y. I just want to say – take one quick \\ndigression to talk about one more algorithm, which was the discussion sort of alluding to \\nthis earlier, which is the perceptron algorithm, right? So I’m not gonna say a whole lot \\nabout the perceptron algorithm, but this is some thing that we’ll come back to later. Later \\nthis quarter we’ll talk about le arning theory. So in logistic re gression we said that G of Z \\nare, sort of, my hypothesis output values th at were low numbers between zero and one. \\nThe question is what if you want to force G of  Z to up the value to either zero one? So the \\nperceptron algorithm defines G of Z to be this. So the picture is – or  the cartoon is, rather \\nthan this sigmoid function. E of Z now looks like this step function that you were asking '),\n",
       " Document(metadata={'page': 2.0, 'source': 'data/MachineLearning-Lecture02.pdf'}, page_content='Instructor (Andrew Ng) :All right, so who thought driving could be that dramatic, right? \\nSwitch back to the chalkboard, please. I s hould say, this work was done about 15 years \\nago and autonomous driving has come a long way. So many of you will have heard of the \\nDARPA Grand Challenge, where one of my colleagues, Sebastian Thrun, the winning \\nteam\\'s drive a car across a desert by itself.  \\nSo Alvin was, I think, absolutely amazing wo rk for its time, but autonomous driving has \\nobviously come a long way since then. So what  you just saw was an example, again, of \\nsupervised learning, and in particular it was an  example of what they  call the regression \\nproblem, because the vehicle is trying to predict a continuous value variables of a \\ncontinuous value steering directions , we call the regression problem.  \\nAnd what I want to do today is talk about our first supervised learning algorithm, and it \\nwill also be to a regression task. So for the running example that I\\'m going to use \\nthroughout today\\'s lecture, you\\'re going to retu rn to the example of  trying to predict \\nhousing prices. So here\\'s actually a data set collected by TA, Dan Ramage, on housing \\nprices in Portland, Oregon.  \\nSo here\\'s a dataset of a number of houses of different sizes, and here are their asking \\nprices in thousands of dollars, $200,000. And so we  can take this data and plot it, square \\nfeet, best price, and so you make your other dataset like that. And the question is, given a \\ndataset like this, or given what we call a tr aining set like this, how  do you learn to predict \\nthe relationship between th e size of the house and th e price of the house?  \\nSo I\\'m actually going to come back and modify th is task a little bit mo re later, but let me \\ngo ahead and introduce some notation, which I\\'ll be using, actually, th roughout the rest of \\nthis course. The first piece of notation is I\\'m going to let the lower case alphabet M \\ndenote the number of training examples, and that just means the number of rows, or the \\nnumber of examples, houses, and prices we have.  \\nAnd in this particular data set, we have, what actually happens, we have 47 training \\nexamples, although I wrote down only five. Oka y, so throughout this quarter, I\\'m going \\nto use the alphabet M to denot e the number of training examples. I\\'m going to use the \\nlower case alphabet X to denote the input variable s, which I\\'ll often also call the features. \\nAnd so in this case, X would denote the size of the house they were looking at.  \\nI\\'m going to use Y to denote the \"output\" va riable, which is sometimes also called a \\ntarget variable, and so one pair, x, y, is wh at comprises one training example. In other \\nwords, one row on the table I drew just now  what would be what I call one training \\nexample, and the Ith training example, in othe r words the Ith row in that table, I\\'m going \\nto write as XI, Y, I.  \\nOkay, and so in this notation they\\'re going to  use this superscript I is not exponentiation. \\nSo this is not X to the power of IY to the pow er of I. In this notation, the superscript I in \\nparentheses is just sort of an index into the Ith row of my list of training examples.  '),\n",
       " Document(metadata={'page': 17.0, 'source': 'data/MachineLearning-Lecture02.pdf'}, page_content=\"algebra. Okay, some of you look a little bi t dazed, but this is our first learning hour. \\nAren't you excited? Any quick questions a bout this before we close for today?  \\nStudent: [Inaudible].  \\nInstructor (Andrew Ng) :Say that again.  \\nStudent: What you derived, wasn't that just [inaudible] of X?  \\nInstructor (Andrew Ng) :What inverse? \\nStudent: Pseudo inverse.  \\nInstructor (Andrew Ng) :Pseudo inverse?  \\nStudent: Pseudo inverse.  \\nInstructor (Andrew Ng) :Yeah, I turns out that in cases, if X transpose X is not \\ninvertible, than you use the pseudo inverse mi nimized to solve this. But it turns out X \\ntranspose X is not invertible. That usually m eans your features were dependent. It usually \\nmeans you did something like repeat the same feat ure twice in your training set. So if this \\nis not invertible, it turns out the minimu m is obtained by the pseudo inverses of the \\ninverse.  \\nIf you don't know what I just sa id, don't worry about it. It usually won't be a problem. \\nAnything else?  \\nStudent: On the second board [inaudible]?  \\nInstructor (Andrew Ng) :Let me take that off. We're running over. Let's close for today \\nand if they're other questions, I'll take them after.  \\n[End of Audio]  \\nDuration: 79 minutes \")]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What did they say about regression in the third lecture?\"\n",
    "vectorstore.similarity_search(question) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
